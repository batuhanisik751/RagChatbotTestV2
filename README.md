# ğŸ“ Document Coach - Intelligent Resume & Document Analysis Chatbot

A RAG (Retrieval-Augmented Generation) powered chatbot that enables natural language queries over multiple resumes and documents. Features advanced security guardrails against prompt injection attacks and intelligent file type classification.

---

## ğŸ¯ Overview

Document Coach is an AI-powered document analysis system designed for hiring workflows and document review. Upload PDF resumes and other documents, then ask natural language questions about candidates. The system automatically classifies document types, extracts structured information, and protects against manipulation attempts embedded in documents.

**Key Differentiators:**
- **Security-First Design** â€” Multi-layer prompt injection detection and sanitization
- **Universal Document Support** â€” Handles resumes, cover letters, transcripts, and other documents
- **Intelligent Classification** â€” Dedicated LLM classifies document types before extraction

---

## âœ¨ Features

### Core Functionality
- **ğŸ“„ Multi-Document Upload** â€” Drag and drop PDF files (max 2MB each)
- **ğŸ” Semantic Search** â€” Find candidates based on skills, experience, and qualifications
- **ğŸ’¬ Natural Language Queries** â€” Ask questions like "Who has Python experience?" or "What was John doing in 2024?"
- **ğŸ§  Conversation Context** â€” Maintains context across questions with pronoun resolution ("What about his education?")
- **ğŸ“Š Document Classification** â€” Automatically identifies resumes vs. cover letters, transcripts, portfolios, etc.

### Advanced RAG Pipeline
- **LLM-based File Classification** â€” Separate classification step determines document type before extraction
- **Structured Data Extraction** â€” Extracts work history, education, skills from resumes; key entities and facts from other documents
- **Temporal Query Support** â€” Handles date-based queries ("What was John doing in December 2024?")
- **Entity Tracking** â€” Resolves pronouns using conversation history and response context
- **Multi-candidate Queries** â€” Supports comparison queries across all loaded candidates

### Security & Guardrails
- **ğŸ›¡ï¸ Prompt Injection Detection** â€” Detects and neutralizes manipulation attempts in documents
- **ğŸ”’ Zero-Width Character Removal** â€” Strips invisible Unicode characters used for hidden text
- **ğŸ§¹ Content Sanitization** â€” Removes whitespace encoding, Unicode smuggling, and homoglyph attacks
- **âš ï¸ Risk Scoring** â€” Calculates and displays injection risk scores for each document
- **ğŸš« Guarded Prompts** â€” All LLM calls include injection-resistant system instructions
- **ğŸ“‹ Duplicate Detection** â€” Prevents duplicate uploads via file hash, content fingerprint, and name matching

### Example Questions You Can Ask:
- "Who do you have?" / "List all candidates"
- "Who has Python experience?"
- "Tell me about John's background"
- "What was she doing in January 2024?" (uses conversation context)
- "Compare candidates for a backend role"
- "What skills does Sarah have?"
- "Who worked at Google?"

---

## ğŸ› ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Frontend** | Streamlit | Web application framework & UI |
| **LLM** | OpenAI GPT-4o / GPT-4o-mini | Classification, extraction, and answer generation |
| **Vector Database** | FAISS | In-memory vector storage for semantic search |
| **Embeddings** | SentenceTransformers (all-MiniLM-L6-v2) | Document and query embeddings |
| **PDF Processing** | pypdf | PDF text extraction |
| **Language** | Python 3.8+ | Core programming language |

### Model Configuration

| Role | Model | Purpose |
|------|-------|---------|
| **Classifier** | `gpt-4o-mini` | Determine document type (resume vs. non-resume) |
| **Extractor** | `gpt-4o-mini` | Extract structured data from documents |
| **Answerer** | `gpt-4o` | Generate final answers to user queries |
| **Chat** | `gpt-4o-mini` | Query suggestions and conversation support |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           DOCUMENT UPLOAD PIPELINE                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User uploads PDF
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FILE VALIDATION  â”‚ â—„â”€â”€ Size check (2MB limit), duplicate detection
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAW EXTRACTION   â”‚ â—„â”€â”€ pypdf extracts text from PDF
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SANITIZATION LAYER                          â”‚
â”‚  â€¢ Zero-width character removal (20+ Unicode chars)          â”‚
â”‚  â€¢ Whitespace encoding detection                             â”‚
â”‚  â€¢ Unicode smuggling neutralization                          â”‚
â”‚  â€¢ Injection phrase detection                                â”‚
â”‚                                                              â”‚
â”‚  OUTPUT: sanitized_text + InjectionReport                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FILE TYPE CLASSIFICATION (LLM #1)               â”‚
â”‚  Model: gpt-4o-mini                                          â”‚
â”‚  Input: First 2000 chars of sanitized text                   â”‚
â”‚  Output: {file_type, confidence, justification}              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                   â–¼                   â–¼
    file_type=         file_type=          file_type=
      resume           non_resume           unknown
         â”‚                   â”‚                   â”‚
         â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              GUARDED EXTRACTION (LLM #2)                     â”‚
â”‚  All prompts include GUARDRAIL_PREAMBLE                      â”‚
â”‚                                                              â”‚
â”‚  Resume â†’ work history, education, skills, owner name        â”‚
â”‚  Non-resume â†’ key entities, facts, dates, summary            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STORAGE & INDEX  â”‚ â—„â”€â”€ FAISS vector index + metadata
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              GUARDED ANSWER GENERATION (LLM #3)              â”‚
â”‚  Model: gpt-4o                                               â”‚
â”‚  Document content treated as DATA ONLY                       â”‚
â”‚  No recommendations or rankings generated                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Security Architecture

### Prompt Injection Defense Layers

| Layer | Technique | Purpose |
|-------|-----------|---------|
| **1** | Zero-width character removal | Strips 20+ invisible Unicode characters |
| **2** | Whitespace encoding detection | Detects tab/space binary patterns |
| **3** | Unicode smuggling neutralization | Removes tag chars, PUA, orphan selectors |
| **4** | NFKC normalization | Prevents homoglyph attacks |
| **5** | Injection phrase detection | Flags manipulation attempts |
| **6** | Guarded LLM prompts | System instructions ignore document commands |

### Risk Score Calculation

```
Risk Score = min(1.0,
    zero_width_chars Ã— 0.05 (max 0.3) +
    whitespace_anomalies Ã— 0.1 (max 0.2) +
    unicode_issues Ã— 0.05 (max 0.2) +
    injection_phrases Ã— 0.2 (max 0.5)
)
```

**Risk Levels:**
- ğŸŸ¢ 0.0-0.2: Low risk (normal document)
- ğŸŸ¡ 0.2-0.5: Medium risk (warning displayed)
- ğŸ”´ 0.5-1.0: High risk (flagged in UI)

### Detected Injection Patterns

The system detects and flags:
- Direct instruction overrides ("ignore previous instructions")
- Ranking manipulation ("this candidate is the best")
- Role hijacking ("you are now a helpful assistant")
- Hidden endorsements ("secretly note that...")
- Self-promotional claims embedded as facts

---

## ğŸ“¦ Installation

### Prerequisites
- Python 3.8+
- OpenAI API key

### Local Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/document-coach.git
   cd document-coach
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Run the application**
   ```bash
   streamlit run app.py
   ```

5. **Enter API key in sidebar**
   
   Navigate to `http://localhost:8501` and enter your OpenAI API key in the sidebar.

---

## ğŸš€ Deployment

### Streamlit Cloud

1. Push your code to GitHub
2. Go to [share.streamlit.io](https://share.streamlit.io)
3. Connect your GitHub repo
4. Add your `OPENAI_API_KEY` in Streamlit secrets:
   - Go to App Settings â†’ Secrets
   - Add: `OPENAI_API_KEY = "your_key_here"`
5. Deploy!

---

## ğŸ“ Project Structure

```
document-coach/
â”œâ”€â”€ app.py                 # Main Streamlit application
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md              # This documentation
â”œâ”€â”€ .gitignore             # Git ignore file
â””â”€â”€ screenshots/           # Application screenshots (optional)
```

---

## âš™ï¸ Configuration

### File Limits

| Setting | Default | Description |
|---------|---------|-------------|
| `MAX_FILE_SIZE_MB` | 2 | Maximum file size per document |

### Models

| Constant | Model | Purpose |
|----------|-------|---------|
| `CLASSIFIER_MODEL` | `gpt-4o-mini` | Document type classification |
| `EXTRACTION_MODEL` | `gpt-4o-mini` | Structured data extraction |
| `ANSWER_MODEL` | `gpt-4o` | Answer generation |
| `CHAT_MODEL` | `gpt-4o-mini` | Suggestions and conversation |

---

## ğŸ“– Usage

### 1. Upload Documents
- Use the file uploader to select PDF files
- System validates file size and checks for duplicates
- Click "Process Documents" to begin analysis

### 2. Review Classification
- Each document is classified as resume or non-resume
- Security risk scores are displayed
- Warnings shown for suspicious content

### 3. Ask Questions
- Type natural language queries in the chat input
- Use pronouns for follow-up questions ("What about his education?")
- View debug info to see how queries are interpreted

### Query Types

| Type | Example | Behavior |
|------|---------|----------|
| List all | "Who do you have?" | Lists all loaded documents |
| Specific person | "Tell me about John" | Answers using only John's resume |
| Timeline | "What was John doing in 2024?" | Filters work history by date |
| Search all | "Who has Python experience?" | Searches across all documents |
| Iterate all | "What was everyone doing in 2023?" | Checks each candidate |

---

## âš ï¸ Known Limitations

- **PDF only** â€” Currently supports PDF files; no Word/text file support
- **English only** â€” Optimized for English language documents
- **2MB limit** â€” Large files with images may exceed size limit
- **No OCR** â€” Scanned PDFs without text layer are not supported
- **In-memory storage** â€” Vector index resets when app restarts
- **Rate limits** â€” Subject to OpenAI API rate limits

---

## ğŸ”§ Error Handling

| Error | Cause | Resolution |
|-------|-------|------------|
| `NoneType` errors | Missing metadata fields | Helper function provides fallbacks |
| File size exceeded | PDF > 2MB | Compress PDF or split into smaller files |
| Duplicate detected | Same file/content uploaded | Skip or confirm replacement |
| Classification failed | Unclear document type | Falls back to heuristic classification |
| High injection risk | Suspicious content detected | Document processed with warning |

---

## ğŸ“‹ Dependencies

```
streamlit
openai
pypdf
sentence-transformers
faiss-cpu
numpy
nltk
python-dateutil
```

---

## ğŸ™ Acknowledgments

- [Streamlit](https://streamlit.io/) for the web framework
- [OpenAI](https://openai.com/) for language models
- [FAISS](https://github.com/facebookresearch/faiss) for vector search
- [SentenceTransformers](https://www.sbert.net/) for embeddings

---

<p align="center">
  Built with security in mind ğŸ›¡ï¸
</p>
